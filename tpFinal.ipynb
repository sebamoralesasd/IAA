{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPFinal.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyQ9hDUA4AaO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 1"
      ],
      "metadata": {
        "id": "iw8XINCa3UNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_images, test_labels):\n",
        "  test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
        "  print(f\"Error en el conjunto de test: {1 - test_acc}\")"
      ],
      "metadata": {
        "id": "Uly1cYYn6M6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_errores(history, model):\n",
        "  error_train = list(map(lambda x: 1-x, history.history['accuracy']))\n",
        "  error_val = list(map(lambda x: 1-x,  history.history['val_accuracy']))\n",
        "\n",
        "  plt.plot(error_train, label='Error train')\n",
        "  plt.plot(error_val, label = 'Error validacion')\n",
        "  plt.xlabel('Épocas')\n",
        "  plt.ylabel('Error')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qnYDENHs6nNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(dataset):\n",
        "  (train_images, train_labels), (test_images, test_labels) = dataset.load_data()\n",
        "\n",
        "  # Normalize pixel values to be between 0 and 1\n",
        "  train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "  return train_images, train_labels, test_images, test_labels"
      ],
      "metadata": {
        "id": "pMNQP0Sc5KK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ej1_fit(model):\n",
        "  train_images, train_labels, test_images, test_labels = get_dataset(datasets.cifar10)\n",
        "\n",
        "  train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "      train_images, train_labels, test_size=0.2, random_state=0\n",
        "      )\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_images, train_labels, epochs=10, \n",
        "                      validation_data=(val_images, val_labels))\n",
        "  \n",
        "  return history, model, test_images, test_labels"
      ],
      "metadata": {
        "id": "huePSKux2jpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ej1():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "\n",
        "  model.add(layers.Dense(64))\n",
        "  model.add(layers.Dense(128))\n",
        "  model.add(layers.Dense(128))\n",
        "  model.add(layers.Dense(10))\n",
        "\n",
        "  #history, fit_model, test_images, test_labels = ej1_fit(model)\n",
        "\n",
        "  return ej1_fit(model)"
      ],
      "metadata": {
        "id": "qadOMQgJ3yp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history, fit_model, test_images, test_labels = ej1()"
      ],
      "metadata": {
        "id": "qZMzPwyD76C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  graph_errores(history, fit_model)\n",
        "  evaluate(fit_model, test_images, test_labels)"
      ],
      "metadata": {
        "id": "RZVFGdkm76c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con este modelo, se logró un error de aproximadamente 0.31 sobre el conjunto de test. A partir del gráfico se puede observar también que se produjo sobreajuste en las primeras épocas. "
      ],
      "metadata": {
        "id": "cIvTesWu--TL"
      }
    }
  ]
}